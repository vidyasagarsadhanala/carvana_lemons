{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capstone 1 - Machine Learning – Predicting Lemon titles (Kicks) in Car Auctions\n",
    "#### Objective:\n",
    "> To predict if the car purchased at the auction is a good/bad buy among thousands of cars purchased through online auctions. The goal is to create a machine learning model to predict the condition of the vehicle being purchased at a car auction, if it is a good/bad buy, hence reducing the risk.  \n",
    "\n",
    "#### Problem:\n",
    "> Predict if the car being purchased at auction is Good or Bad buy?\n",
    "\n",
    "#### Outcome:\n",
    ">One of the challenges for an auto dealership in purchasing a used car at an auction is the risk of that vehicle might have serious issues that prevent it from being resold. These are referred to as “kicks” or unfortunate purchases and are often resulting in a significant loss. Some examples of kicks could be tampered odometers, mechanical issues the dealer is not able to address, issues with getting the vehicle title from the seller or some unforeseen problem. Using machine learning to predict which cars have higher risk can provide real value to dealerships as they can predict kicks before the dealership buys at auctions.\n",
    "\n",
    "#### Dataset:\n",
    ">Source: https://www.kaggle.com/c/DontGetKicked/data\n",
    "\n",
    ">Train set – 60%<br>\n",
    ">Test set – 40%\n",
    "\n",
    ">The data set contains information about each car, like purchase price, make and model, trim level, odometer reading, date of purchase, state of origin and so on. There are about 40 different variables (along with the lemon status indicator IsBadBuy) on around 72K cars, the test data set has the same information on around 40K cars. The target variable is “IsBadBuy” which is a binary variable and is a post-purchase classification for kicked on non-kicked cars.\n",
    "\n",
    "#### Evaluation Metrics:\n",
    ">The evaluation metrics for this problem are going to be the Gini Index, Classification Accuracy %, F1 Score, Precision, Recall, and Log Loss metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## Import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas_profiling\n",
    "from fancyimpute import KNN\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\datawiz\\\\Documents\\\\Springboard\\\\carvana_lemons'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'C:\\\\Users\\\\datawiz\\\\Documents\\\\Springboard\\\\carvana_lemonsdata\\\\training.csv' does not exist: b'C:\\\\Users\\\\datawiz\\\\Documents\\\\Springboard\\\\carvana_lemonsdata\\\\training.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-4626f6daf0d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Input training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:\\\\Users\\\\datawiz\\\\Documents\\\\Springboard\\\\carvana_lemonsdata\\\\training.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mM:\\Python\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mM:\\Python\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mM:\\Python\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mM:\\Python\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mM:\\Python\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'C:\\\\Users\\\\datawiz\\\\Documents\\\\Springboard\\\\carvana_lemonsdata\\\\training.csv' does not exist: b'C:\\\\Users\\\\datawiz\\\\Documents\\\\Springboard\\\\carvana_lemonsdata\\\\training.csv'"
     ]
    }
   ],
   "source": [
    "# Input training data\n",
    "df = pd.read_csv('C:\\\\Users\\\\datawiz\\\\Documents\\\\Springboard\\\\carvana_lemonsdata\\\\training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['IsBadBuy'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['Make'],sort=False)['IsBadBuy'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.hist(df, figsize= [15,15]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop columns:\n",
    "1. Ref ID\n",
    "2. BYRNO\n",
    "3. WheelTypeID\n",
    "\n",
    "##### Numerical Columns:\n",
    "1.  VehYear\n",
    "2.  VehcileAge\n",
    "3.  WarrantyCost\n",
    "4.  VehOdo\n",
    "5.  VehBCost\n",
    "6.  VNZIP1\n",
    "7.  MMRAcquisitionAuctionAveragePrice\n",
    "8.  MMRAcquisitionAuctionCleanPrice\n",
    "9.  MMRAcquisitionRetailAveragePrice\n",
    "11. MMRAcquisitonRetailCleanPrice\n",
    "12. MMRCurrentAuctionAveragePrice\n",
    "13. MMRCurrentAuctionCleanPrice\n",
    "14. MMRCurrentRetailAveragePrice\n",
    "15. MMRCurrentRetailCleanPrice\n",
    "\n",
    "##### Categorical Columns:\n",
    "1. Auction\n",
    "2. Transmission\n",
    "3. WheelType\n",
    "4. Nationality\n",
    "5. Size\n",
    "6. TopThreeAmericanName\n",
    "7. IsOnlineSale\n",
    "\n",
    "##### Fix NULLs\n",
    "1. Trim\n",
    "2. AUCGUART\n",
    "3. PRIMEUNIT\n",
    "4. ALL Price Cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets looks a a data profiling report using pandas_profiling API\n",
    "#pandas_profiling.ProfileReport(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_corr = df.copy()\n",
    "df_new_corr = df_new_corr.drop(['RefId','PurchDate','Auction','Make','Model','Size','TopThreeAmericanName','PRIMEUNIT','AUCGUART',\n",
    "                              'Trim','SubModel','Color','Transmission','WheelType','Nationality','BYRNO','VNST'],axis=1)\n",
    "correlations = df_new_corr.corr()['IsBadBuy'].sort_values()\n",
    "print('Most Positive Correlations: \\n', correlations.tail(5))\n",
    "print('\\nMost Negative Correlations: \\n', correlations.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's plot a heatmap to visualize the correaltion between IsBadBuy and other attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations\n",
    "corr = df_new_corr.corr()\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "# Heatmap\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(corr,\n",
    "            vmax=.5,\n",
    "            mask=mask,\n",
    "            # annot=True, fmt='.2f',\n",
    "            linewidths=.2, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As seen in the profile report above, the price attributes are highliy correlated to each other. <p>\n",
    "> VehAge, WarrantCost and VehOdo are the most correlated attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataprocessing\n",
    "1. Dropping columms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = df\n",
    "# Dropping columns\n",
    "df = df.drop(['PRIMEUNIT', 'AUCGUART'], axis=1)\n",
    "\n",
    "#dropping the target variable\n",
    "#df = df.drop('IsBadBuy', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Has NULL Values:\n",
    "\n",
    ">1. Trim\n",
    ">2. AUCGUART\n",
    ">3. PRIMEUNIT\n",
    ">4. ALL Price Cols\n",
    "\n",
    ">Numerical Variables:\n",
    "\n",
    ">1. VehYear\n",
    ">2. VehicleAge\n",
    ">3. VehOdi\n",
    ">4. VNZIP1\n",
    ">5. VehCost\n",
    ">6. All Price Columns\n",
    "\n",
    ">Categorical Variables:\n",
    "\n",
    ">1. Auction\n",
    ">2. Transmission\n",
    ">3. WheelType\n",
    ">4. Nationality\n",
    ">5. Size\n",
    ">6. TopThreeAmericanName\n",
    ">7. IsOnlineSale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The target variable IsBadBuy a binary classifcation variable, meaning we are assigned a value of 1 if the car purchased in a Bad buy or 0 if a car is not a Bad buy (good buy).\n",
    "\n",
    "> It is important to note that while doing this prediction we need to be careful about the high cost of predicting false negatives. This means that a dealership might think that this car is a good buy and think they would be able to sell it, however in reality this a actually a Bad Buy and not sellable.\n",
    "\n",
    "> A false postive has a cost associated with it as well, if the purchase as classified as a Bad buu in realilty it is indeed a sellable car, then the delearship might loose the opportunity selling the used car and generating profit of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Quesitons:\n",
    "\n",
    ">1.There is no column in the Test data provided from the Kaggle Competetion, does this mean that I have to use Cross Validation sampling to split the Training Data into either 5 folds to 10 folds and also how can I know how many number of folds to choose in this case?\n",
    "\n",
    ">2.WheelType has 3174 / 4.3% missing values Missing\n",
    ">WheelTypeID has 3169 / 4.3% missing values Missing\n",
    ">I thought both should have mostly same missing values, but they do not.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Clarifications:\n",
    ">1. Auction: This is the expected price of the car at an Auction.\n",
    ">2. MMR: This is Manheim Market Report, which is an indicator of wholesale prices of a car determined by a very establised company that provides very statistically sounds whole car price determinations.\n",
    ">3. Acquisition: This is the price at which the car's MMR sold at the auction. \n",
    ">4. Retail: This mean the expected price of the car which the customer is willing to pay at the dealership.\n",
    ">5. VNST and VNZIP1 are state and zip codes\n",
    ">6. TopAmericanName: If the vechicle is from one of the top three american car manufacturers.\n",
    "\n",
    ">nearest neighbhors for categorials or look at similar cars look at high correlated variables understand the missing data and then find solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Make'].value_counts().plot(kind='barh', figsize=(10, 8))\n",
    "plt.xlabel(\"Car Makes\", labelpad=14)\n",
    "plt.ylabel(\"Vehicle Counts\", labelpad=14)\n",
    "plt.title(\"Counts of Car Makes\", y=1.02);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace Manual with MANUAL\n",
    "df['Transmission'].replace(to_replace =['Manual'],\n",
    "                           value =\"MANUAL\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Transmission'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Write a pattern to extract numbers and decimals\n",
    "def return_eng_size(length):\n",
    "    pattern = re.compile(r\"\\d+\\.\\d[lL]\")\n",
    "    \n",
    "    # Search the text for matches\n",
    "    size = re.search(pattern, length)\n",
    "    \n",
    "    # If a value is returned, use group(0) to return the found value\n",
    "    if size is not None:\n",
    "        return str(size.group(0))\n",
    "    else:\n",
    "        return str(\"Missing\")\n",
    "    \n",
    "df[\"EngineSize\"] = df['Model'].apply(lambda x: return_eng_size(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a pattern to extract numbers and decimals\n",
    "def return_veh_char(length):\n",
    "    pattern = re.compile(r\"4WD|2WD|AWD|FWD|V8|V6|4C|6C|DOHC|MPI|SFI*|MFI|EFI*\")\n",
    "    \n",
    "    # Search the text for matches\n",
    "    veh_char = re.search(pattern, str(length))\n",
    "    \n",
    "    # If a value is returned, use group(0) to return the found value\n",
    "    if veh_char is not None:\n",
    "        return str(veh_char.group(0))\n",
    "    else:\n",
    "        return str(\"Missing\")\n",
    "    \n",
    "df[\"VehileEngChar\"] = df['Model'].apply(lambda x: return_veh_char(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"VehileEngCharSub\"] = df['SubModel'].apply(lambda x: return_veh_char(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"VehileEngChar\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['VehEngChar'] = df.apply(lambda x: 'AWD' if x['VehileEngChar'] == 'AWD' or x['VehileEngCharSub'] == 'AWD' else x['VehileEngChar'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['VehEngChar'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['VehileEngChar','VehileEngCharSub'],axis=1)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a pattern to extract numbers and decimals\n",
    "def return_eng_cylnd(length):\n",
    "    pattern = re.compile(r\"([vViI]\\d|[vViI]\\s\\d|[vViI]\\-\\d)\")\n",
    "    \n",
    "    # Search the text for matches\n",
    "    eng_cylnd = re.search(pattern, length)\n",
    "    \n",
    "    # If a value is returned, use group(0) to return the found value\n",
    "    if eng_cylnd is not None:\n",
    "        return str(eng_cylnd.group(0))\n",
    "    else:\n",
    "        return str(\"Missing\")\n",
    "    \n",
    "df[\"VehileEngCylinder\"] = df['Model'].apply(lambda x: return_eng_cylnd(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.EngineSize.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_eng_vals(x):\n",
    "    if x == \"I-4\":\n",
    "        return str(\"I4\")\n",
    "    elif x == \"I 4\":\n",
    "        return str(\"I4\")\n",
    "    elif x == \"I-2\":\n",
    "        return str(\"I2\")\n",
    "    elif x == \"I 2\":\n",
    "        return str(\"I2\")\n",
    "    elif x == \"I-3\":\n",
    "        return str(\"I3\")\n",
    "    elif x == \"I 3\":\n",
    "        return str(\"I3\")\n",
    "    elif x == \"I-6\":\n",
    "        return str(\"I6\")\n",
    "    elif x == \"I 6\":\n",
    "        return str(\"I6\")\n",
    "    elif x == \"V-6\":\n",
    "        return str(\"V6\")\n",
    "    elif x == \"V 6\":\n",
    "        return str(\"V6\")\n",
    "    elif x == \"V-4\":\n",
    "        return str(\"V4\")\n",
    "    elif x == \"V 4\":\n",
    "        return str(\"V4\")\n",
    "    elif x == \"V-2\":\n",
    "        return str(\"V2\")\n",
    "    elif x == \"V 2\":\n",
    "        return str(\"V2\")\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "df[\"VehEngCylndr\"] = df['VehileEngCylinder'].apply(lambda x: fix_eng_vals(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"VehEngCylndr\"].value_counts()\n",
    "df.drop(['VehileEngCylinder'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Model'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The data set contains several numerical and categorial attributes\n",
    "df.columns.to_series().groupby(df.dtypes).groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_df = df[['MMRCurrentRetailCleanPrice','MMRCurrentRetailAveragePrice','MMRCurrentAuctionCleanPrice',\n",
    "            'MMRCurrentAuctionAveragePrice','MMRAcquisitionRetailAveragePrice', 'MMRAcquisitionAuctionCleanPrice',\n",
    "             'MMRAcquisitonRetailCleanPrice','MMRAcquisitionAuctionAveragePrice','Color','SubModel',\n",
    "             'TopThreeAmericanName','Nationality','Size']].isnull().sum().sort_values(ascending=False)\n",
    "print(null_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressing Current Prices based on Acquisition Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num = df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "clf = linear_model.LinearRegression()\n",
    "degree = 3\n",
    "polynomial_features = PolynomialFeatures(degree = degree, include_bias = True)\n",
    "pipeline = Pipeline([(\"polynomial_features\", polynomial_features), (\"classifier\", clf)])\n",
    "\n",
    "non_null_train = df_num[(pd.notnull(df_num.MMRAcquisitionAuctionAveragePrice)) &\n",
    "  (pd.notnull(df_num.MMRAcquisitionAuctionCleanPrice)) &\n",
    "  (pd.notnull(df_num.MMRAcquisitionRetailAveragePrice)) &\n",
    "  (pd.notnull(df_num.MMRAcquisitonRetailCleanPrice)) &\n",
    "  (pd.notnull(df_num.MMRCurrentAuctionAveragePrice)) &\n",
    "  (pd.notnull(df_num.MMRCurrentAuctionCleanPrice)) &\n",
    "  (pd.notnull(df_num.MMRCurrentRetailAveragePrice)) &\n",
    "  (pd.notnull(df_num.MMRCurrentRetailCleanPrice))]\n",
    "\n",
    "null_mask = ((pd.notnull(df_num.MMRAcquisitionAuctionAveragePrice)) &\n",
    "  (pd.notnull(df_num.MMRAcquisitionAuctionCleanPrice)) &\n",
    "  (pd.notnull(df_num.MMRAcquisitionRetailAveragePrice)) &\n",
    "  (pd.notnull(df_num.MMRAcquisitonRetailCleanPrice)) &\n",
    "  (pd.isnull(df_num.MMRCurrentAuctionAveragePrice)) &\n",
    "  (pd.isnull(df_num.MMRCurrentAuctionCleanPrice)) &\n",
    "  (pd.isnull(df_num.MMRCurrentRetailAveragePrice)) &\n",
    "  (pd.isnull(df_num.MMRCurrentRetailCleanPrice)))\n",
    "\n",
    "null_predict = df_num[null_mask]\n",
    "\n",
    "X = non_null_train[['MMRAcquisitionAuctionAveragePrice',\n",
    "  'MMRAcquisitionAuctionCleanPrice',\n",
    "  'MMRAcquisitionRetailAveragePrice',\n",
    "  'MMRAcquisitonRetailCleanPrice',\n",
    "  'VehicleAge',\n",
    "  'VehOdo'\n",
    "]]\n",
    "\n",
    "X_predict = null_predict[['MMRAcquisitionAuctionAveragePrice',\n",
    "  'MMRAcquisitionAuctionCleanPrice',\n",
    "  'MMRAcquisitionRetailAveragePrice',\n",
    "  'MMRAcquisitonRetailCleanPrice',\n",
    "  'VehicleAge',\n",
    "  'VehOdo'\n",
    "]]\n",
    "\n",
    "for target in ['MMRCurrentAuctionAveragePrice', \n",
    "               'MMRCurrentAuctionCleanPrice', \n",
    "               'MMRCurrentRetailAveragePrice', \n",
    "               'MMRCurrentRetailCleanPrice']:\n",
    "    y = non_null_train[target]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size= 0.3, random_state=123)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    print(pipeline.score(X_train, y_train))\n",
    "    print(pipeline.score(X_test, y_test))\n",
    "    result = pipeline.predict(X_predict)\n",
    "    df_num.loc[null_mask, target] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num[['MMRCurrentAuctionAveragePrice', \n",
    "        'MMRCurrentAuctionCleanPrice', \n",
    "        'MMRCurrentRetailAveragePrice', \n",
    "        'MMRCurrentRetailCleanPrice']].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing the remaining missing numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import Imputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "num_df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "imputer.fit(num_df)\n",
    "\n",
    "X = imputer.transform(num_df)\n",
    "\n",
    "num_df = pd.DataFrame(X, columns=num_df.columns.values)\n",
    "\n",
    "num_df.head()\n",
    "\n",
    "num_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the price columns are related as per the report above.\n",
    "avg_prices_cols = ['MMRAcquisitionAuctionAveragePrice',\n",
    "              'MMRAcquisitionAuctionCleanPrice',\n",
    "              'MMRAcquisitionRetailAveragePrice',\n",
    "              'MMRAcquisitonRetailCleanPrice',              \n",
    "              'MMRCurrentAuctionAveragePrice',\n",
    "              'MMRCurrentAuctionCleanPrice',\n",
    "              'MMRCurrentRetailAveragePrice',\n",
    "              'MMRCurrentRetailCleanPrice']\n",
    "\n",
    "#df['AvgAuctionPrice'] = sum(df[i] for i in avg_prices_cols) / len(avg_prices_cols) \n",
    "#df = df.drop(avg_prices_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_trim_df = df[df['Trim'].isnull()]\n",
    "empty_trim_df['Make'].value_counts().plot(kind='barh',figsize=(10, 8))\n",
    "plt.xlabel(\"Count of Car Makes with Nulls\", labelpad=14)\n",
    "plt.ylabel(\"Car Makes\", labelpad=14)\n",
    "plt.title(\"Count Null values by Car Manufacturer\", y=1.02);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Suzuki cars have a lot of their Trim values missing in the data set.\n",
    "> Will be using categorical imputation for filling missing values by most common occurance by using fancy impute package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a copy of the data frame so far\n",
    "clean_df = df.copy()\n",
    "\n",
    "#Extract date features and drop the date column\n",
    "clean_df['PurchDate'] = pd.to_datetime(clean_df['PurchDate'])\n",
    "\n",
    "# Adding new features from the Purchase date\n",
    "clean_df['PurchDay'] = clean_df['PurchDate'].apply(lambda x:x.day)\n",
    "clean_df['PurchMon'] = clean_df['PurchDate'].apply(lambda x:x.month)\n",
    "clean_df['PurchYear'] = clean_df['PurchDate'].apply(lambda x:x.year)\n",
    "\n",
    "#Remove columns deemed not necessary since we already extracted its features\n",
    "clean_df = clean_df.drop(['PurchDate'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values in the dataset:\")\n",
    "print(clean_df.isnull().sum())\n",
    "print(\"\\n\")\n",
    "print(\"% of missing values:\")\n",
    "print(clean_df.isnull().mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of Missing Values using missingno package in Python\n",
    "import missingno as msno\n",
    "\n",
    "#Plot the missing no bar chart\n",
    "msno.bar(clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the matrix to show \n",
    "msno.matrix(clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the heat to show any correlation between missing values, for example, wheel type\n",
    "msno.heatmap(clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Dendrogram for futher analysis for distance between like missing values\n",
    "msno.dendrogram(clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a copy of the data frame for Categrorical imputation\n",
    "cars_df = clean_df.copy(deep=True)\n",
    "\n",
    "# Drop the columns that are not needed like numerical and categorical values that are fully existing in the data set.\n",
    "cars_df = cars_df.drop(['Auction','VehYear','VehicleAge','Make','Model','VehOdo','BYRNO','VNZIP1','VNST',\n",
    "                        'VehBCost','IsOnlineSale','WarrantyCost','WheelTypeID','IsBadBuy','MMRAcquisitionAuctionAveragePrice',\n",
    "                       'MMRAcquisitionAuctionCleanPrice','MMRAcquisitionRetailAveragePrice','MMRAcquisitonRetailCleanPrice',\n",
    "                        'MMRCurrentAuctionAveragePrice','MMRCurrentAuctionCleanPrice','MMRCurrentRetailAveragePrice',\n",
    "                       'MMRCurrentRetailCleanPrice'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical variables selected for imputation\n",
    "cars_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary ordinal_enc_dict\n",
    "ordinal_enc_dict = {}\n",
    "\n",
    "for col_name in cars_df:\n",
    "    # Create Ordinal encoder for col\n",
    "    ordinal_enc_dict[col_name] = OrdinalEncoder()\n",
    "    col = cars_df[col_name]\n",
    "    \n",
    "    # Select non-null values of col\n",
    "    col_not_null = col[col.notnull()]\n",
    "    reshaped_vals = col_not_null.values.reshape(-1, 1)\n",
    "    encoded_vals = ordinal_enc_dict[col_name].fit_transform(reshaped_vals)\n",
    "    \n",
    "    # Store the values to non-null values of the column in users\n",
    "    cars_df.loc[col.notnull(), col_name] = np.squeeze(encoded_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_df.info()\n",
    "from fancyimpute import SimpleFill\n",
    "# Create SimpleFill imputer\n",
    "Simple_imp = SimpleFill(\"mean\")\n",
    "\n",
    "# Impute and round the users DataFrame\n",
    "#clean_df.iloc[:, :] = np.round(KNN_imputer.fit_transform(clean_df))\n",
    "cars_df.iloc[:, :] = np.round(Simple_imp.fit_transform(cars_df))\n",
    "\n",
    "\n",
    "# Loop over the column names in users\n",
    "for col_name in cars_df:\n",
    "    \n",
    "    # Reshape the data\n",
    "    reshaped = cars_df[col_name].values.reshape(-1, 1)\n",
    "    \n",
    "    # Perform inverse transform of the ordinally encoded columns\n",
    "    cars_df[col_name] = ordinal_enc_dict[col_name].inverse_transform(reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View to the imputed data frame\n",
    "cars_df.head(5)\n",
    "\n",
    "# merging the numerical and categorical variables data frames\n",
    "final_df = pd.merge(num_df, cars_df, on='RefId')\n",
    "\n",
    "# Variables in the final data frame before addressing class imbalance\n",
    "final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since over 95% values are missing in PRIMEUNIT and AUCGUART variables, removing these columns\n",
    "#final_df = final_df.drop(['PRIMEUNIT','AUCGUART'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the class imbalance in the data set\n",
    "df_grpd = final_df.groupby(['IsBadBuy']).size().plot.barh(x=\"IsBadBuy\",y=\"Vehicle Counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import package\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_majority = final_df[final_df.IsBadBuy==0]\n",
    "df_minority = final_df[final_df.IsBadBuy==1]\n",
    " \n",
    "# Upsample minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=64007,    # to match majority class\n",
    "                                 random_state=1234) # reproducible results\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    " \n",
    "# Display new class counts\n",
    "df_upsampled.IsBadBuy.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grpd_upsam = df_upsampled.groupby(['IsBadBuy']).size().plot.barh(x=\"IsBadBuy\",y=\"Vehicle Counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy upsmaples DataFrame\n",
    "data = df_upsampled.copy(deep=True)\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection\n",
    "#### dropping WheelTypeID since WheelTypeName exists \n",
    "#### dropping ID variable RefID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_upsampled.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy upsmaples DataFrame\n",
    "data = df_upsampled.copy(deep=True)\n",
    "\n",
    "# Dropping varibles that are not neeeded and also target variables\n",
    "X = data.drop(['RefId','WheelTypeID','IsBadBuy'], axis=1)\n",
    "\n",
    "# target variable\n",
    "y = data.IsBadBuy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=123, \n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dummy variables over entire train/test set\n",
    "def make_dummies(train_in, test_in, feature):\n",
    "    from sklearn import preprocessing\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    train_set = list(set(train_in[feature].values))\n",
    "    test_set = list(set(test_in[feature].values))\n",
    "    encoder_set = list(set(train_set + test_set))\n",
    "    le.fit(encoder_set)\n",
    "    new_feature = le.transform(train_in[feature].values)\n",
    "    train_in=train_in.drop(feature, axis=1)\n",
    "    train_in[feature] = new_feature \n",
    "    new_feature = le.transform(test_in[feature].values)\n",
    "    test_in=test_in.drop(feature, axis=1)\n",
    "    test_in[feature] = new_feature\n",
    " \n",
    "    return test_in, train_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(actual, pred):\n",
    "    assert (len(actual) == len(pred))\n",
    "    all = np.asarray(np.c_[actual, pred, np.arange(len(actual))], dtype=np.float)\n",
    "    all = all[np.lexsort((all[:, 2], -1 * all[:, 1]))]\n",
    "    totalLosses = all[:, 0].sum()\n",
    "    giniSum = all[:, 0].cumsum().sum() / totalLosses\n",
    "\n",
    "    giniSum -= (len(actual) + 1) / 2.\n",
    "    return giniSum / len(actual)\n",
    "\n",
    "\n",
    "def gini_normalized(actual, pred):\n",
    "    return gini(actual, pred) / gini(actual, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ['Trim','SubModel', 'Color' ,'Transmission', 'WheelType', 'Nationality',\n",
    "                'Size', 'TopThreeAmericanName', 'EngineSize' ,'VehEngChar' ,'VehEngCylndr']:\n",
    "    X_test, X_train = make_dummies(X_train, X_test, feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate a DecisionTreeClassifier 'dt' with a maximum depth of 6\n",
    "dt = DecisionTreeClassifier(criterion ='gini', max_depth=30, random_state=1)\n",
    "\n",
    "# Fit dt to the training set\n",
    "dt.fit(X_train,y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Compute test set accuracy  \n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Test set accuracy: {:.2f}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Generate the confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "\n",
    "gini_predictions = gini(y_test, y_pred)\n",
    "gini_max = gini(y_test, y_pred)\n",
    "ngini= gini_normalized(y_test, y_pred)\n",
    "print('Gini: %.3f, Max. Gini: %.3f, Normalized Gini: %.3f' % (gini_predictions, gini_max, ngini))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KNeighborsClassifier from sklearn.neighbors\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "\n",
    "# Create a k-NN classifier with 6 neighbors: knn\n",
    "knn = KNeighborsClassifier(n_neighbors=4)\n",
    "\n",
    "# Fit the classifier to the data\n",
    "knn.fit(X_train,y_train)\n",
    "\n",
    "# Predict the labels for the training data X_test\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "\n",
    "# Accuracy Scores\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred_knn))\n",
    "\n",
    "# Generate the confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, y_pred_knn))\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "\n",
    "f1_knn = metrics.f1_score(y_pred_knn, y_test)\n",
    "\n",
    "print(\"F1 Score:\"+str(f1_knn))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "gini_predictions_knn = gini(y_test, y_pred_knn)\n",
    "gini_max_knn = gini(y_test, y_pred_knn)\n",
    "ngini_knn = gini_normalized(y_test, y_pred_knn)\n",
    "print('Gini: %.3f, Max. Gini: %.3f, Normalized Gini: %.3f' % (gini_predictions_knn, gini_max_knn, ngini_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RF = RandomForestClassifier(n_estimators = 50, criterion = 'entropy', min_samples_split = 285)\n",
    "\n",
    "RF.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = RF.predict(X_test)\n",
    "\n",
    "f1_rf = metrics.f1_score(y_pred_rf, y_test)\n",
    "\n",
    "print(\"F1 Score:\"+str(f1_rf))\n",
    "\n",
    "metrics.classification_report(y_test, y_pred_rf)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred_rf))\n",
    "\n",
    "importance = zip(RF.feature_importances_, X)\n",
    "\n",
    "for rank in sorted(importance, key = lambda x: x[0], reverse = True):\n",
    "    print(rank)\n",
    "    \n",
    "print(\"\\n\")\n",
    "\n",
    "gini_predictions_rf = gini(y_test, y_pred_rf)\n",
    "gini_max_rf = gini(y_test, y_pred_rf)\n",
    "ngini_rf = gini_normalized(y_test, y_pred_rf)\n",
    "print('Gini: %.3f, Max. Gini: %.3f, Normalized Gini: %.3f' % (gini_predictions_rf, gini_max_rf, ngini_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# instansitate xbg classifier\n",
    "xg_cl = xgb.XGBRFClassifier(objective='binary:logistic', n_estimators=10,seed=123)\n",
    "\n",
    "# Fit on train\n",
    "xg_cl.fit(X_train, y_train)\n",
    "\n",
    "#Predict on Test\n",
    "y_pred_xgb = xg_cl.predict(X_test)\n",
    "\n",
    "#Accuracy Scores\n",
    "accuracy_xgb = float(np.sum(y_pred_xgb==y_test))/y_test.shape[0]\n",
    "\n",
    "print('accuracy: %f' % (accuracy_xgb))\n",
    "\n",
    "f1_xgb = metrics.f1_score(y_pred_xgb, y_test)\n",
    "\n",
    "print(\"F1 Score:\"+str(f1_xgb))\n",
    "\n",
    "metrics.classification_report(y_test, y_pred_xgb)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred_xgb))\n",
    "\n",
    "importance = zip(xg_cl.feature_importances_, X)\n",
    "\n",
    "for rank in sorted(importance, key = lambda x: x[0], reverse = True):\n",
    "    print(rank)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "gini_predictions_xgb = gini(y_test, y_pred_xgb)\n",
    "gini_max_xgb = gini(y_test, y_pred_xgb)\n",
    "ngini_xgb = gini_normalized(y_test, y_pred_xgb)\n",
    "print('Gini: %.3f, Max. Gini: %.3f, Normalized Gini: %.3f' % (gini_predictions_xgb, gini_max_xgb, ngini_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on the different machine learning models applied so far for classifiying good and bar car buys at the auction, DecisionTreeClassifier seems be the most performant model of all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
